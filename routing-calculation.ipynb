{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"/kaggle/working/prueba\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-06T08:46:55.683261Z","iopub.execute_input":"2023-06-06T08:46:55.684685Z","iopub.status.idle":"2023-06-06T08:46:55.701338Z","shell.execute_reply.started":"2023-06-06T08:46:55.684567Z","shell.execute_reply":"2023-06-06T08:46:55.699855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ccardenas93/Routing_Runoff.git","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:26:36.239542Z","iopub.execute_input":"2023-06-06T09:26:36.240650Z","iopub.status.idle":"2023-06-06T09:26:38.848361Z","shell.execute_reply.started":"2023-06-06T09:26:36.240608Z","shell.execute_reply":"2023-06-06T09:26:38.847144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nfrom rasterio.mask import mask\nfrom datetime import datetime, timedelta","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:26:44.205657Z","iopub.execute_input":"2023-06-06T09:26:44.206931Z","iopub.status.idle":"2023-06-06T09:26:44.787901Z","shell.execute_reply.started":"2023-06-06T09:26:44.206873Z","shell.execute_reply":"2023-06-06T09:26:44.786603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basin_station_map = {\n    'WOL4': 'P11',\n    'WOL5': 'P16',\n    'WOL7': 'P13',\n    'MAL1': 'P14',\n    'ZEN6': 'P14',\n    'ZEN3': 'P07',\n    'BNV4': 'P07',\n    'BNV8': 'P10',\n    'MOL5': 'P02',# Replace 'Pxxx' with the actual station name\n    # Add more basin-station mappings as required\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:26:47.779375Z","iopub.execute_input":"2023-06-06T09:26:47.779788Z","iopub.status.idle":"2023-06-06T09:26:47.786134Z","shell.execute_reply.started":"2023-06-06T09:26:47.779737Z","shell.execute_reply":"2023-06-06T09:26:47.785035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TIA_20m = rasterio.open(r\"/kaggle/working/Routing_Runoff/INPUTS/TIA_20mf.tif\")\nLAI_max_20m = rasterio.open(r\"/kaggle/working/Routing_Runoff/INPUTS/LAI_max_20mf.tif\")\nLAI_min_20m = rasterio.open(r\"/kaggle/working/Routing_Runoff/INPUTS/LAI_min_20mf.tif\")\n\nprint(TIA_20m)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:26:52.740780Z","iopub.execute_input":"2023-06-06T09:26:52.741180Z","iopub.status.idle":"2023-06-06T09:26:53.006631Z","shell.execute_reply.started":"2023-06-06T09:26:52.741150Z","shell.execute_reply":"2023-06-06T09:26:53.005492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for basin_name, station in basin_station_map.items():\n    # Update input file paths\n    basin_shp_path = f\"/kaggle/working/Routing_Runoff/{basin_name}/basin.shp\"\n    # Load the shapefile, rasters, and CSV file\n    basin_gdf = gpd.read_file(basin_shp_path)\n\n\n    precip_df = pd.read_csv(r\"/kaggle/working/Routing_Runoff/INPUTS/9modeledStns.csv\", parse_dates=['date'])\n\n    # Get the geometry of the shapefile in GeoJSON format and mask the TIA and LAI rasters\n    basin_geom = basin_gdf.geometry.to_crs(TIA_20m.crs)\n    TIA_array, TIA_transform = mask(TIA_20m, basin_geom, crop=True, nodata=TIA_20m.nodata)\n    LAI_max_array, LAI_max_transform = mask(LAI_max_20m, basin_geom, crop=True, nodata=LAI_max_20m.nodata)\n    LAI_min_array, _ = mask(LAI_min_20m, basin_geom, crop=True, nodata=LAI_min_20m.nodata)\n\n    # Squeeze the arrays to remove the unnecesary dimension and set nodata values to NaN\n    TIA_array = np.squeeze(TIA_array).astype('float32')\n    LAI_max_array = np.squeeze(LAI_max_array).astype('float32')\n    LAI_min_array = np.squeeze(LAI_min_array).astype('float32')\n    TIA_array[TIA_array == TIA_20m.nodata] = np.nan\n    LAI_max_array[LAI_max_array == LAI_max_20m.nodata] = np.nan\n    LAI_min_array[LAI_min_array == LAI_min_20m.nodata] = np.nan\n\n    print(f\"Processing basin: {basin_name}\")\n    \n    start_date = '01/01/2016'  # Replace with the desired start date 'MM/DD/YYYY'\n    end_date = '12/31/2018'    # Replace with teh desired end date 'MM/DD/YYYY'\n\n    # Filter the precipitation data based on user input\n    precip_df = precip_df[(precip_df['date'] >= start_date) & (precip_df['date'] <= end_date) & (precip_df[station] > 0)][['date', station]]\n    print(precip_df)\n    # Function to calculate Pnet based on LAI and precipitation\n    def calculate_pnet(LAI, P):\n        if np.isnan(LAI) or np.isnan(P):\n            return np.nan\n\n        if LAI < 1:\n            Pnet = 0.04 + 0.99 * (P*12) - 0.09 * LAI\n        else:\n            if P < 0.6:\n                Pnet = 0.03 + 0.72 * (P*12) - 0.05 * np.log(LAI)\n            else:\n                Pnet = -0.02 + 0.98 * (P*12) - 0.09 * LAI\n        \n        return min(Pnet, P)\n\n    # Vectorize the Pnet calculation function\n    calculate_pnet_vec = np.vectorize(calculate_pnet)\n\n    # Calculate Pnet for each date and precipitation value with a progress bar\n    pnet_arrays = []\n    for index, row in tqdm(precip_df.iterrows(), total=precip_df.shape[0]):\n        P = row[station]\n        month = row['date'].month\n\n        # Select the appropriate LAI raster based on the month\n        if month in [3, 4, 5, 6, 7, 8]:  # Spring and summer months\n            LAI_array = LAI_max_array\n        else:  # Autumn and winter months\n            LAI_array = LAI_min_array\n\n        Pnet_array = calculate_pnet_vec(LAI_array, P)\n        pnet_arrays.append(Pnet_array)\n\n    # Save Pnet arrays to a list in the DataFrame\n    precip_df['Pnet_arrays'] = pnet_arrays\n\n    # TIA_array = TIA_20m.read(1)\n    # TIA_array[TIA_array < 0] = 0  # Set values less than 0 to 0\n\n    # Calculate RO for each Pnet array\n    ro_arrays = []\n    for pnet_array in pnet_arrays:\n        ro_array = (pnet_array * (0.01 * pnet_array + 0.47 * TIA_array + 0.02))\n        ro_arrays.append(ro_array)\n\n    # Save RO arrays to a list in the DataFrame\n    precip_df['RO_arrays'] = ro_arrays\n\n    # Read the TO raster and get the geometry of the shapefile in GeoJSON format\n    TO = rasterio.open(f\"/kaggle/working/Routing_Runoff/INPUTS/TO/{basin_name}TO.tif\")\n    basin_geom = basin_gdf.geometry.to_crs(TO.crs)\n    TO_array, TO_transform = mask(TO, basin_geom, crop=True, nodata=TO.nodata)\n\n    # Squeeze the arrays to remove the unnecessary dimension and set nodata values to NaN\n    TO_array = np.squeeze(TO_array).astype('float32')\n    TO_array[TO_array == TO.nodata] = np.nan\n\n    # Convert TO_array from hours to minutes\n    TO_array = TO_array * 60\n\n    # Reclassify the TO_array to 5-minute intervals\n    TO_array = np.ceil(TO_array / 5)\n\n    # Initialize an empty DataFrame for the runoff time series\n    runoff_time_series = pd.DataFrame(columns=['date', 'RO'])\n\n    # Iterate through each RO_array in precip_df\n    for _, row in precip_df.iterrows():\n        RO_array = row['RO_arrays']\n        date = row['date']\n\n        # Get the unique values in the TO array\n        unique_TO_values = np.unique(TO_array)\n\n        # Iterate through the unique values in the TO array\n        for TO_value in unique_TO_values:\n            # Find the positions in the TO array where the value equals TO_value\n            positions = np.where(TO_array == TO_value)\n\n            # Sum the values in the RO_array at the matching positions\n            RO_sum = (((np.sum(RO_array[positions])/12)*400)/(1000*300))\n\n            # Increment the timestamp by 5 minutes for each unique TO value\n            date += timedelta(minutes=5)\n\n            # Append the sum and the corresponding timestamp to the runoff_time_series DataFrame\n            runoff_time_series = runoff_time_series.append({'date': date, 'RO': RO_sum}, ignore_index=True)\n\n # Convert the date column to datetime, group by date, and sum the RO values for each group\n    runoff_time_series['date'] = pd.to_datetime(runoff_time_series['date'])\n    runoff_time_series = runoff_time_series.groupby('date').sum().reset_index()\n\n    # Set the date column as the index and sort the DataFrame by index\n    runoff_time_series = runoff_time_series.set_index('date').sort_index()\n\n    # Create a complete DateTimeIndex for the desired date range with a 5-minute frequency\n    start_datetime = datetime.strptime(start_date, '%m/%d/%Y')\n    end_datetime = datetime.strptime(end_date, '%m/%d/%Y')\n    complete_index = pd.date_range(start=start_datetime, end=end_datetime, freq='5min')\n\n    # Reindex the DataFrame with the complete DateTimeIndex and fill missing values with 0\n    runoff_time_series = runoff_time_series.reindex(complete_index, fill_value=0)\n\n    # Filter the DataFrame based on the inputted date range\n    start_date_str = start_datetime.strftime('%Y-%m-%d')\n    end_date_str = end_datetime.strftime('%Y-%m-%d')\n    filtered_runoff_time_series = runoff_time_series.loc[start_date_str:end_date_str]\n\n    # Export filtered_runoff_time_series to a CSV file\n    output_file_path = f\"/kaggle/working/Routing_Runoff/{basin_name}/{basin_name}_routed_RO_transform.csv\"\n    filtered_runoff_time_series.to_csv(output_file_path, index=True)\n\n    print(f\"Filtered runoff time series for {basin_name} has been saved as {output_file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:27:26.721504Z","iopub.execute_input":"2023-06-06T09:27:26.721916Z","iopub.status.idle":"2023-06-06T09:28:21.669654Z","shell.execute_reply.started":"2023-06-06T09:27:26.721883Z","shell.execute_reply":"2023-06-06T09:28:21.668318Z"},"trusted":true},"execution_count":null,"outputs":[]}]}